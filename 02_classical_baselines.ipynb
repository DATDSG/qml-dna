{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9906c5a9",
   "metadata": {},
   "source": [
    "# 02_classical_baselines.ipynb ‚Äî SVMs \n",
    "\n",
    "This notebook runs classical SVM baselines on the prepared encodings. The BLAS/perf cell must be kept as-is and run first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6ff2f8",
   "metadata": {},
   "source": [
    "# Cell 0 ‚Äî perf env (keep as-is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35a46d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLAS: 8 8\n"
     ]
    }
   ],
   "source": [
    "# Standardize BLAS thread usage to reduce run-to-run variability\n",
    "import os\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"8\")\n",
    "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"8\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"8\")\n",
    "os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"8\")\n",
    "print(\"BLAS:\", os.environ.get(\"OMP_NUM_THREADS\"), os.environ.get(\"OPENBLAS_NUM_THREADS\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3298e3",
   "metadata": {},
   "source": [
    "# Cell 1 ‚Äî load data (robust, multi-dataset aware + journaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05ffe5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ [load] Loaded encodings from encodings_all.npz and splits from splits_pooled.json\n",
      "‚úÖ [datasets] Detected 13 dataset(s) with mapping from dataset_index.csv\n",
      "Split sizes: 12336 4112 4112 | pos rate: 0.8654182879377432\n",
      "‚úÖ [splits] train=12336, val=4112, test=4112, pos_rate=0.8654\n"
     ]
    }
   ],
   "source": [
    "# Load encoded sequence representations and train/val/test indices\n",
    "from pathlib import Path\n",
    "import json, warnings, time\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, roc_auc_score,\n",
    "    confusion_matrix, classification_report, balanced_accuracy_score,\n",
    "    matthews_corrcoef, average_precision_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "PROCESSED = Path(\"data/processed\")\n",
    "RESULTS = Path(\"results\")\n",
    "(RESULTS/\"metrics\").mkdir(parents=True, exist_ok=True)\n",
    "(RESULTS/\"plots\").mkdir(parents=True, exist_ok=True)\n",
    "(RESULTS/\"logs\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Run journal (for documentation) ----\n",
    "class RunJournal:\n",
    "    def __init__(self): self.events = []\n",
    "    def log(self, step, status, message, **extras):\n",
    "        self.events.append({\n",
    "            \"ts\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"step\": step, \"status\": status, \"message\": message, **extras\n",
    "        })\n",
    "        sym = \"‚úÖ\" if status==\"ok\" else (\"‚ö†Ô∏è\" if status==\"warn\" else \"‚ùå\")\n",
    "        print(f\"{sym} [{step}] {message}\")\n",
    "    def df(self): return pd.DataFrame(self.events)\n",
    "    def save(self, base: Path):\n",
    "        df = self.df()\n",
    "        md = [\"| ts | step | status | message |\", \"|---|---|---|---|\"]\n",
    "        for _,r in df.iterrows():\n",
    "            md.append(f\"| {r.ts} | {r.step} | {r.status} | {r.message} |\")\n",
    "        (base.with_suffix(\".md\")).write_text(\"\\n\".join(md), encoding=\"utf-8\")\n",
    "        (base.with_suffix(\".json\")).write_text(df.to_json(orient=\"records\", indent=2), encoding=\"utf-8\")\n",
    "        print(f\"üìù Saved journal:\\n  - {base.with_suffix('.md')}\\n  - {base.with_suffix('.json')}\")\n",
    "\n",
    "J = RunJournal()\n",
    "\n",
    "# ---- Resolve artifact filenames (new multi-dataset first; fallback to original) ----\n",
    "enc_candidates = [PROCESSED/\"encodings_all.npz\", PROCESSED/\"encodings.npz\"]\n",
    "spl_candidates = [PROCESSED/\"splits_pooled.json\", PROCESSED/\"splits.json\"]\n",
    "\n",
    "enc_path = next((p for p in enc_candidates if p.exists()), None)\n",
    "spl_path = next((p for p in spl_candidates if p.exists()), None)\n",
    "\n",
    "if enc_path is None or spl_path is None:\n",
    "    if enc_path is None: J.log(\"load\", \"fail\", \"Encodings file not found (tried encodings_all.npz, encodings.npz)\")\n",
    "    if spl_path is None: J.log(\"load\", \"fail\", \"Splits file not found (tried splits_pooled.json, splits.json)\")\n",
    "    raise FileNotFoundError(\"Required data artifacts missing in data/processed\")\n",
    "\n",
    "data = np.load(enc_path, allow_pickle=True)\n",
    "with open(spl_path) as f:\n",
    "    SPL = json.load(f)\n",
    "\n",
    "J.log(\"load\", \"ok\", f\"Loaded encodings from {enc_path.name} and splits from {spl_path.name}\")\n",
    "\n",
    "# Arrays\n",
    "y = data[\"y\"]\n",
    "X_kmer   = data[\"kmer\"]\n",
    "X_onehot = data[\"onehot\"]\n",
    "# Optional fields (multi-dataset)\n",
    "ds_idx = data[\"ds_idx\"] if \"ds_idx\" in data.files else None\n",
    "dataset_index_csv = PROCESSED/\"dataset_index.csv\"\n",
    "ds_map = None\n",
    "if ds_idx is not None and dataset_index_csv.exists():\n",
    "    ds_map = pd.read_csv(dataset_index_csv).set_index(\"ds_idx\")[\"accession\"].to_dict()\n",
    "    J.log(\"datasets\", \"ok\", f\"Detected {len(set(ds_idx))} dataset(s) with mapping from dataset_index.csv\")\n",
    "elif ds_idx is not None:\n",
    "    J.log(\"datasets\", \"warn\", \"ds_idx present but dataset_index.csv missing ‚Äî per-dataset names unavailable\")\n",
    "\n",
    "# Splits\n",
    "tr, va, te = map(np.array, (SPL[\"train\"], SPL[\"val\"], SPL[\"test\"]))\n",
    "pos_rate = float(y.mean()) if len(y) else float(\"nan\")\n",
    "print(\"Split sizes:\", len(tr), len(va), len(te), \"| pos rate:\", pos_rate)\n",
    "J.log(\"splits\", \"ok\", f\"train={len(tr)}, val={len(va)}, test={len(te)}, pos_rate={pos_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312b573e",
   "metadata": {},
   "source": [
    "# Cell 2 ‚Äî evaluation helpers (full metrics + logging + saves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a66c3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Utility: compute extended metrics from predictions ---\n",
    "def extended_metrics(y_true, y_prob, thr):\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_hat, average=\"binary\", zero_division=0)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_prob)\n",
    "    except Exception:\n",
    "        auc = float(\"nan\")\n",
    "    try:\n",
    "        ap = average_precision_score(y_true, y_prob)  # PR-AUC\n",
    "    except Exception:\n",
    "        ap = float(\"nan\")\n",
    "    cm = confusion_matrix(y_true, y_hat, labels=[0,1])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    tnr = tn / (tn + fp) if (tn + fp) else float(\"nan\")  # specificity\n",
    "    bal_acc = balanced_accuracy_score(y_true, y_hat)\n",
    "    mcc = matthews_corrcoef(y_true, y_hat) if len(np.unique(y_true))==2 else float(\"nan\")\n",
    "    rep = classification_report(y_true, y_hat, output_dict=True, zero_division=0)\n",
    "    return {\n",
    "        \"acc\": acc, \"prec\": prec, \"rec\": rec, \"f1\": f1,\n",
    "        \"roc_auc\": auc, \"pr_auc\": ap, \"specificity\": tnr,\n",
    "        \"balanced_acc\": bal_acc, \"mcc\": mcc, \"thr\": thr,\n",
    "        \"tp\": int(tp), \"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn),\n",
    "        \"support\": int(len(y_true)),\n",
    "    }, cm, rep\n",
    "\n",
    "# --- Threshold search on validation to maximize F1 ---\n",
    "def choose_threshold(y_val, p_val, name=\"model\"):\n",
    "    grid = np.linspace(0.05, 0.95, 37)\n",
    "    best_thr, best_f1 = 0.5, -1\n",
    "    for t in grid:\n",
    "        y_hat = (p_val >= t).astype(int)\n",
    "        from sklearn.metrics import f1_score\n",
    "        f1 = f1_score(y_val, y_hat, zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_thr = float(f1), float(t)\n",
    "    if np.isnan(best_f1):\n",
    "        J.log(\"threshold\", \"warn\", f\"{name}: F1 undefined on val; using default thr=0.5\")\n",
    "        best_thr = 0.5\n",
    "    else:\n",
    "        J.log(\"threshold\", \"ok\", f\"{name}: selected thr={best_thr:.2f} (val F1={best_f1:.3f})\")\n",
    "    return best_thr\n",
    "\n",
    "# --- Plotting helpers (1 figure per chart) ---\n",
    "def plot_roc(y_true, y_prob, title, out_png):\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    try:\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, label=f\"AUC={roc_auc:.3f}\")\n",
    "        plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "        plt.xlabel(\"FPR\")\n",
    "        plt.ylabel(\"TPR\")\n",
    "        plt.title(title)\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(out_png, dpi=150)\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        J.log(\"plot\", \"warn\", f\"ROC plot skipped: {e}\")\n",
    "\n",
    "def plot_pr(y_true, y_prob, title, out_png):\n",
    "    from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "    try:\n",
    "        prec, rec, _ = precision_recall_curve(y_true, y_prob)\n",
    "        ap = average_precision_score(y_true, y_prob)\n",
    "        plt.figure()\n",
    "        plt.plot(rec, prec, label=f\"AP={ap:.3f}\")\n",
    "        plt.xlabel(\"Recall\")\n",
    "        plt.ylabel(\"Precision\")\n",
    "        plt.title(title)\n",
    "        plt.legend(loc=\"lower left\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(out_png, dpi=150)\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        J.log(\"plot\", \"warn\", f\"PR plot skipped: {e}\")\n",
    "\n",
    "def save_cm_csv(cm, out_csv, normalized=False):\n",
    "    if normalized:\n",
    "        cm = cm.astype(np.float64)\n",
    "        row_sums = cm.sum(axis=1, keepdims=True)\n",
    "        cm = np.divide(cm, np.where(row_sums==0, 1, row_sums))\n",
    "    df = pd.DataFrame(cm, index=[\"true_0\",\"true_1\"], columns=[\"pred_0\",\"pred_1\"])\n",
    "    df.to_csv(out_csv, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905846ef",
   "metadata": {},
   "source": [
    "# Cell 3 ‚Äî SVM (k-mer) with full documentation outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1c876b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ [fit] SVM_kmer: fitting on train (k-mer, n=12336)\n",
      "‚úÖ [threshold] SVM_kmer: selected thr=0.40 (val F1=0.969)\n",
      "acc             0.916829\n",
      "prec            0.922956\n",
      "rec             0.986232\n",
      "f1              0.953545\n",
      "roc_auc         0.801506\n",
      "pr_auc          0.938734\n",
      "specificity     0.470163\n",
      "balanced_acc    0.728197\n",
      "mcc             0.590643\n",
      "thr                  0.4\n",
      "tp                  3510\n",
      "tn                   260\n",
      "fp                   293\n",
      "fn                    49\n",
      "support             4112\n",
      "model           SVM_kmer\n",
      "split               test\n",
      "‚úÖ [eval] SVM_kmer: test F1=0.954, AUC=0.802, PR-AUC=0.939, thr=0.40\n"
     ]
    }
   ],
   "source": [
    "# Radial basis SVM on normalized k-mer frequency vectors\n",
    "model_name = \"SVM_kmer\"\n",
    "svm_kmer = make_pipeline(\n",
    "    StandardScaler(with_mean=True, with_std=True),\n",
    "    SVC(C=5.0, kernel=\"rbf\", gamma=\"scale\", probability=True, class_weight=\"balanced\", random_state=0)\n",
    ")\n",
    "\n",
    "J.log(\"fit\", \"ok\", f\"{model_name}: fitting on train (k-mer, n={len(tr)})\")\n",
    "svm_kmer.fit(X_kmer[tr], y[tr])\n",
    "\n",
    "# Threshold selection on validation\n",
    "p_val = svm_kmer.predict_proba(X_kmer[va])[:, 1]\n",
    "thr = choose_threshold(y[va], p_val, name=model_name)\n",
    "\n",
    "# Evaluate splits\n",
    "split_data = {\"train\": (X_kmer[tr], y[tr]), \"val\": (X_kmer[va], y[va]), \"test\": (X_kmer[te], y[te])}\n",
    "rows, reports = [], {}\n",
    "cms = {}\n",
    "for split, (Xsplit, ysplit) in split_data.items():\n",
    "    p = svm_kmer.predict_proba(Xsplit)[:, 1]\n",
    "    m, cm, rep = extended_metrics(ysplit, p, thr)\n",
    "    m.update({\"model\": model_name, \"split\": split})\n",
    "    rows.append(m); cms[split] = cm; reports[split] = rep\n",
    "\n",
    "df_metrics = pd.DataFrame(rows)\n",
    "df_metrics.to_csv(RESULTS / \"metrics/svm_kmer_metrics.csv\", index=False)\n",
    "\n",
    "# Save confusion matrices (raw + normalized)\n",
    "for split, cm in cms.items():\n",
    "    save_cm_csv(cm, RESULTS / f\"metrics/svm_kmer_cm_{split}.csv\", normalized=False)\n",
    "    save_cm_csv(cm, RESULTS / f\"metrics/svm_kmer_cm_{split}_norm.csv\", normalized=True)\n",
    "\n",
    "# Save classification reports (per split)\n",
    "with open(RESULTS / \"metrics/svm_kmer_classification_reports.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(reports, f, indent=2)\n",
    "\n",
    "# Plots (test split)\n",
    "plot_roc(y[te], svm_kmer.predict_proba(X_kmer[te])[:, 1], f\"{model_name} ‚Äî ROC (test)\", RESULTS / \"plots/svm_kmer_roc_test.png\")\n",
    "plot_pr (y[te], svm_kmer.predict_proba(X_kmer[te])[:, 1], f\"{model_name} ‚Äî PR (test)\",  RESULTS / \"plots/svm_kmer_pr_test.png\")\n",
    "\n",
    "# Console + journal summary (avoid backslashes in f-strings)\n",
    "row_test = df_metrics.loc[df_metrics[\"split\"] == \"test\"].iloc[0]\n",
    "print(row_test.to_string())\n",
    "\n",
    "J.log(\n",
    "    \"eval\", \"ok\",\n",
    "    f\"{model_name}: test F1={row_test['f1']:.3f}, \"\n",
    "    f\"AUC={row_test['roc_auc']:.3f}, \"\n",
    "    f\"PR-AUC={row_test['pr_auc']:.3f}, thr={thr:.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47564ef",
   "metadata": {},
   "source": [
    "# Cell 4 ‚Äî SVM (one-hot flattened) with full documentation outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f1aac0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ [fit] SVM_onehot: fitting on train (one-hot flat, n=12336, d=1024)\n",
      "‚úÖ [threshold] SVM_onehot: selected thr=0.30 (val F1=0.978)\n",
      "acc               0.917558\n",
      "prec              0.917315\n",
      "rec                0.99438\n",
      "f1                0.954294\n",
      "roc_auc           0.842139\n",
      "pr_auc            0.956038\n",
      "specificity       0.423146\n",
      "balanced_acc      0.708763\n",
      "mcc               0.591716\n",
      "thr                    0.3\n",
      "tp                    3539\n",
      "tn                     234\n",
      "fp                     319\n",
      "fn                      20\n",
      "support               4112\n",
      "model           SVM_onehot\n",
      "split                 test\n",
      "‚úÖ [eval] SVM_onehot: test F1=0.954, AUC=0.842, PR-AUC=0.956, thr=0.30\n"
     ]
    }
   ],
   "source": [
    "# RBF SVM on flattened one-hot sequence encoding (high-dimensional)\n",
    "model_name = \"SVM_onehot\"\n",
    "X_flat = X_onehot.reshape(len(X_onehot), -1).astype(np.float32)\n",
    "\n",
    "svm_1hot = make_pipeline(\n",
    "    StandardScaler(with_mean=True, with_std=True),\n",
    "    SVC(C=2.0, kernel=\"rbf\", gamma=\"scale\", probability=True, class_weight=\"balanced\", random_state=0)\n",
    ")\n",
    "\n",
    "J.log(\"fit\", \"ok\", f\"{model_name}: fitting on train (one-hot flat, n={len(tr)}, d={X_flat.shape[1]})\")\n",
    "svm_1hot.fit(X_flat[tr], y[tr])\n",
    "\n",
    "# Threshold selection on validation\n",
    "p_val = svm_1hot.predict_proba(X_flat[va])[:, 1]\n",
    "thr = choose_threshold(y[va], p_val, name=model_name)\n",
    "\n",
    "# Evaluate splits\n",
    "split_data = {\"train\": (X_flat[tr], y[tr]), \"val\": (X_flat[va], y[va]), \"test\": (X_flat[te], y[te])}\n",
    "rows, reports = [], {}\n",
    "cms = {}\n",
    "for split, (Xsplit, ysplit) in split_data.items():\n",
    "    p = svm_1hot.predict_proba(Xsplit)[:, 1]\n",
    "    m, cm, rep = extended_metrics(ysplit, p, thr)\n",
    "    m.update({\"model\": model_name, \"split\": split})\n",
    "    rows.append(m); cms[split] = cm; reports[split] = rep\n",
    "\n",
    "df_metrics = pd.DataFrame(rows)\n",
    "df_metrics.to_csv(RESULTS / \"metrics/svm_onehot_metrics.csv\", index=False)\n",
    "\n",
    "# Save confusion matrices (raw + normalized)\n",
    "for split, cm in cms.items():\n",
    "    save_cm_csv(cm, RESULTS / f\"metrics/svm_onehot_cm_{split}.csv\", normalized=False)\n",
    "    save_cm_csv(cm, RESULTS / f\"metrics/svm_onehot_cm_{split}_norm.csv\", normalized=True)\n",
    "\n",
    "# Save classification reports (per split)\n",
    "with open(RESULTS / \"metrics/svm_onehot_classification_reports.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(reports, f, indent=2)\n",
    "\n",
    "# Plots (test split)\n",
    "plot_roc(y[te], svm_1hot.predict_proba(X_flat[te])[:, 1], f\"{model_name} ‚Äî ROC (test)\", RESULTS / \"plots/svm_onehot_roc_test.png\")\n",
    "plot_pr (y[te], svm_1hot.predict_proba(X_flat[te])[:, 1], f\"{model_name} ‚Äî PR (test)\",  RESULTS / \"plots/svm_onehot_pr_test.png\")\n",
    "\n",
    "# Console + journal summary (avoid backslashes in f-strings)\n",
    "row_test = df_metrics.loc[df_metrics[\"split\"] == \"test\"].iloc[0]\n",
    "print(row_test.to_string())\n",
    "\n",
    "J.log(\n",
    "    \"eval\", \"ok\",\n",
    "    f\"{model_name}: test F1={row_test['f1']:.3f}, \"\n",
    "    f\"AUC={row_test['roc_auc']:.3f}, \"\n",
    "    f\"PR-AUC={row_test['pr_auc']:.3f}, thr={thr:.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a799e5",
   "metadata": {},
   "source": [
    "# (Optional) Cell 5 ‚Äî Per-dataset diagnostics (if ds_idx available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0636a1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ [per-dataset] SVM_kmer per-dataset metrics saved for 13 dataset(s).\n",
      "‚úÖ [per-dataset] SVM_onehot per-dataset metrics saved for 13 dataset(s).\n"
     ]
    }
   ],
   "source": [
    "if ds_idx is None:\n",
    "    J.log(\"per-dataset\", \"warn\", \"ds_idx not found in encodings ‚Äî skipping per-dataset evaluation.\")\n",
    "else:\n",
    "    # Use pooled best thresholds (per model) to measure generalization per dataset on TEST split only\n",
    "    def per_dataset_eval(model, X_all, model_name, thr):\n",
    "        rows = []\n",
    "        uniq = sorted(np.unique(ds_idx))\n",
    "        for d in uniq:\n",
    "            name = ds_map.get(d, f\"ds_{d}\") if ds_map else f\"ds_{d}\"\n",
    "            mask = (ds_idx[te] == d)  # restrict to test subset for that dataset\n",
    "            if mask.sum() == 0:\n",
    "                continue\n",
    "            y_true = y[te][mask]\n",
    "            y_prob = model.predict_proba(X_all[te][mask])[:,1]\n",
    "            m, cm, _ = extended_metrics(y_true, y_prob, thr)\n",
    "            m.update({\"dataset\": name, \"n\": int(mask.sum()), \"model\": model_name})\n",
    "            rows.append(m)\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    # Reuse thresholds chosen earlier\n",
    "    # For SVM_kmer\n",
    "    thr_kmer = pd.read_csv(RESULTS/\"metrics/svm_kmer_metrics.csv\").query(\"split=='val'\")[\"thr\"].values[0]\n",
    "    df_per_kmer = per_dataset_eval(svm_kmer, X_kmer, \"SVM_kmer\", thr_kmer)\n",
    "    df_per_kmer.to_csv(RESULTS/\"metrics/svm_kmer_per_dataset_test.csv\", index=False)\n",
    "    J.log(\"per-dataset\", \"ok\", f\"SVM_kmer per-dataset metrics saved for {len(df_per_kmer)} dataset(s).\")\n",
    "\n",
    "    # For SVM_onehot\n",
    "    thr_1hot = pd.read_csv(RESULTS/\"metrics/svm_onehot_metrics.csv\").query(\"split=='val'\")[\"thr\"].values[0]\n",
    "    df_per_1hot = per_dataset_eval(svm_1hot, X_flat, \"SVM_onehot\", thr_1hot)\n",
    "    df_per_1hot.to_csv(RESULTS/\"metrics/svm_onehot_per_dataset_test.csv\", index=False)\n",
    "    J.log(\"per-dataset\", \"ok\", f\"SVM_onehot per-dataset metrics saved for {len(df_per_1hot)} dataset(s).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0fc8bb",
   "metadata": {},
   "source": [
    "# Cell 6 ‚Äî Save run journal (what worked, what didn‚Äôt, and why)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b601dbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RUN SUMMARY ===\n",
      "No warnings or failures.\n",
      "üìù Saved journal:\n",
      "  - results\\logs\\classical_baselines_20250918_105341.md\n",
      "  - results\\logs\\classical_baselines_20250918_105341.json\n",
      "üì¶ Metrics in: results\\metrics  |  Plots in: results\\plots\n"
     ]
    }
   ],
   "source": [
    "ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "base = RESULTS/\"logs\"/f\"classical_baselines_{ts}\"\n",
    "# Add a brief ‚Äúlimitations / problems seen‚Äù roll-up for your documentation\n",
    "issues = []\n",
    "for e in J.events:\n",
    "    if e[\"status\"] in (\"warn\",\"fail\"):\n",
    "        issues.append(f\"- [{e['step']}] {e['message']}\")\n",
    "rollup = \"No warnings or failures.\" if not issues else \"Issues observed:\\n\" + \"\\n\".join(issues)\n",
    "print(\"\\n=== RUN SUMMARY ===\\n\" + rollup)\n",
    "\n",
    "J.save(base)\n",
    "(RESULTS/\"logs\"/f\"classical_baselines_{ts}_summary.txt\").write_text(rollup, encoding=\"utf-8\")\n",
    "print(f\"üì¶ Metrics in: {RESULTS/'metrics'}  |  Plots in: {RESULTS/'plots'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
